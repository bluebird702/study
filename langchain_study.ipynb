{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJro/uP3MCu4KAm4GhWXGV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluebird702/study/blob/main/langchain_study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYaGRdqvdguf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 문서는 langchain study를 하면서 정리한 노트입니다."
      ],
      "metadata": {
        "id": "xtL86y2zesDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습 준비\n",
        "colab에서 예제를 실행할 것이므로 Google AI Studio에 계정을 등록하고 Key를 얻는다. 자세한 내용은 다음 문서를 참고한다.\n",
        "\n",
        "[Gemini Key 발급받아오기](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/python_quickstart.ipynb#scrollTo=gHYFrFPjSGNq)\n",
        "\n",
        "# 환경 테스트\n",
        "다음과 같이 테스트해보자. 공식 문서에 있는 내용을 그대로 테스트해본다.\n",
        "\n",
        "[문서 링크](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/python_quickstart.ipynb#scrollTo=FFPBKLapSCkM)\n",
        "\n"
      ],
      "metadata": {
        "id": "ftQ7pupUexag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# google python sdk 설치\n",
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "8hcrFi8yfxwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 왼쪽의 열쇠 버튼에 GOOGLE_API_KEY를 등록한다음에 다음 코드를 돌려서 라이브러리를 초기화\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    display(m.name) # colab에서는 print 말고 display를 써야 wrapping된 결과를 얻을 수 있다.\n"
      ],
      "metadata": {
        "id": "aDExK8Xshlq1",
        "outputId": "9315278d-db44-4f34-f82a-adcf5de0b0b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'models/gemini-1.0-pro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'models/gemini-1.0-pro-001'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'models/gemini-1.0-pro-latest'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'models/gemini-1.0-pro-vision-latest'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'models/gemini-1.5-pro-latest'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'models/gemini-pro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'models/gemini-pro-vision'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 다음의 코드를 테스트해보자\n",
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "zNvUFwMlgQ68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gemini-pro를 일단 사용해본다.\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "QhZvCoMEiynJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time은 맨 첫줄에서만 동작함. 그래서 코드셀을 나눴음.\n",
        "%%time\n",
        "response = model.generate_content(\"삶의 의미가 멀까?\")\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "id": "4XjywuaJivBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stream으로 노출하려면 출력을 다르게 해야한다.\n",
        "%%time\n",
        "response = model.generate_content(\"삶의 의미가 멀까?\", stream=True)\n",
        "for chunk in response:\n",
        "  display(chunk.text)\n",
        "  display(\"_\"*80)"
      ],
      "metadata": {
        "id": "CT6h9ihXjkh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "요정도에서 gemini설정을 마치고 langchain 학습으로 넘어간다.\n",
        "\n",
        "langchain학습은 [테디노트님의 ebook](https://wikidocs.net/book/14314)으로 진행해본다."
      ],
      "metadata": {
        "id": "4iLIZOaWkNEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ch01 LangChain 시작하기 노트\n",
        "1장은 설정을 하고 있는데, openai로 예제가 나와 있다.\n",
        "\n",
        "colab에서 테스트를 쉽게 하고 싶으니 gemini로 바꿔서 테스트해보자.\n",
        "\n",
        "일단 langchain library를 설치한다."
      ],
      "metadata": {
        "id": "gBJVwwISk-7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LangChain 업데이트\n",
        "!pip install -U langchain langchain-community langchain-experimental langchain-core langchain-google-genai langsmith"
      ],
      "metadata": {
        "id": "Wdfxp_jblClX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 잘되는 지 기본 예제 코드를 수행해본다."
      ],
      "metadata": {
        "id": "C84zlChmkovo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# 객체 생성\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# 물어보자.\n",
        "result = llm.invoke(\"넌 누구냐!\")\n",
        "display(f\"[답변]: {result.content}\")"
      ],
      "metadata": {
        "id": "wJ92-378k1p0",
        "outputId": "17600140-9db0-4a40-f170-7bd640aaf365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'[답변]: 저는 Gemini, Google에서 개발한 대규모 다국어 모델입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "프롬프트 템플릿도 한번 써보자.\n",
        "\n",
        "일단 prompt를 만든다."
      ],
      "metadata": {
        "id": "q-t4c2Pim6GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"{country}의 수도는 뭐야?\"\n",
        "\n",
        "# 템플릿 완성\n",
        "prompt = PromptTemplate.from_template(template=template)\n",
        "prompt"
      ],
      "metadata": {
        "id": "YZJEwueRne3k",
        "outputId": "a0b2f8f2-11e2-4c42-832a-fdad64159d77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['country'], template='{country}의 수도는 뭐야?')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그 다음에 prompt를 써서 요청을 날려보자"
      ],
      "metadata": {
        "id": "5YuxfaoIoXSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm) # 위에서 prompt와 llm을 만드는 코드를 실행한 후에 여기를 돌려야 한다.\n",
        "\n",
        "llm_chain.invoke({\"country\": \"대한민국\"}) # 요거는 서울"
      ],
      "metadata": {
        "id": "dpltUWLcn2aa",
        "outputId": "be19832a-dcc4-426b-a172-ff9a727dc1e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'country': '대한민국', 'text': '서울'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "apply()로 여러개의 입력도 한번에 처리가 가능하다."
      ],
      "metadata": {
        "id": "rLpu5fQpoZQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_list = [{\"country\": \"호주\"}, {\"country\": \"중국\"}, {\"country\":\"네덜란드\"}]\n",
        "\n",
        "result = llm_chain.apply(input_list)\n",
        "\n",
        "display(result)\n",
        "\n",
        "for res in result:\n",
        "  display(res[\"text\"].strip())"
      ],
      "metadata": {
        "id": "y8PLy6Qfod1T",
        "outputId": "a406948b-ff47-4217-efab-8b4eb39b241a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[{'text': '캔버라'}, {'text': '베이징'}, {'text': '암스테르담'}]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'캔버라'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'베이징'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'암스테르담'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate()로 좀 더 자세한 추가정보를 출력할 수 있다."
      ],
      "metadata": {
        "id": "h0sF1Uweo-4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_list = [{\"country\": \"호주\"}, {\"country\": \"중국\"}, {\"country\":\"네덜란드\"}]\n",
        "\n",
        "result = llm_chain.generate(input_list)\n",
        "\n",
        "display(result)"
      ],
      "metadata": {
        "id": "ciVXZxkcpGJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰 사용량을 바로 보려면 다음과 같이 찍으면 된다고 하는데 OpenAI는 되나본데, Gemini는 아무것도 안나온다. 따로 방법을 찾아보자."
      ],
      "metadata": {
        "id": "khmIifyLqs3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.llm_output"
      ],
      "metadata": {
        "id": "Y1XNDFOrqxm1",
        "outputId": "234fe11e-e872-441d-dfbc-59848d242758",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2개 이상의 변수를 템플릿 안에 정의하는 것도 가능하다."
      ],
      "metadata": {
        "id": "6tOXnzbNrQcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"{area1}와 {area2}의 시차는 몇시간이야?\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "id": "7UKfHPH3rVxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "KIdklYmOrh4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(llm_chain.invoke({\"area1\": \"서울\", \"area2\": \"파리\"}))"
      ],
      "metadata": {
        "id": "zRa_8MiKrmeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_list = [\n",
        "    {\"area1\": \"파리\", \"area2\": \"뉴욕\"},\n",
        "    {\"area1\": \"서울\", \"area2\": \"하와이\"},\n",
        "    {\"area1\": \"켄버라\", \"area2\": \"베이징\"},\n",
        "]\n",
        "\n",
        "# 반복문으로 결과 출력\n",
        "result = llm_chain.apply(input_list)\n",
        "for res in result:\n",
        "    display(res[\"text\"].strip())"
      ],
      "metadata": {
        "id": "AUlKGqUprsfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "stream으로 출력도 가능하다만 OpenAI로 좀 다른거 같다. stream이란 함수를 호출해서 출력한다."
      ],
      "metadata": {
        "id": "fhMdyVyMsoNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0, #창의성\n",
        "    max_output_tokens=2048,\n",
        ")\n",
        "\n",
        "question = \"대한민국에 대해서 300자 내외로 최대한 상세히 알려줘\"\n",
        "\n",
        "for chunk in llm.stream(question):\n",
        "    display(to_markdown(chunk.content))\n",
        "    display(to_markdown(\"---\"))"
      ],
      "metadata": {
        "id": "4sR3ixAksq-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain 생성에서 LCEL(LangChain Expression Language)도 가능한지 살펴본다."
      ],
      "metadata": {
        "id": "YSQtF6uiuBB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 주어진 나라에 대하여 수도를 묻는 프롬프트 템플릿을 생성합니다.\n",
        "template = \"\"\"\n",
        "당신은 친절하게 답변해 주는 친절 봇입니다. 사용자의 질문에 [FORMAT]에 맞추어 답변해 주세요.\n",
        "답변은 항상 한글로 작성해 주세요.\n",
        "\n",
        "질문:\n",
        "{question}에 대하여 설명해 주세요.\n",
        "\n",
        "FORMAT:\n",
        "- 개요:\n",
        "- 예시:\n",
        "- 출처:\n",
        "\"\"\"\n",
        "\n",
        "template = \"\"\"\n",
        "당신은 영어를 가르치는 10년차 영어 선생님입니다. 상황에 [FORMAT]에 영어 회화를 작성해 주세요.\n",
        "\n",
        "상황:\n",
        "{question}\n",
        "\n",
        "FORMAT:\n",
        "- 영어 회화:\n",
        "- 한글 해석:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# OpenAI 챗모델을 초기화합니다.\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0, #창의성\n",
        "    max_output_tokens=2048,\n",
        ")\n",
        "\n",
        "# 문자열 출력 파서를 초기화합니다.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 프롬프트, 모델, 출력 파서를 연결하여 처리 체인을 구성합니다.\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "# 완성된 Chain 을 이용하여 country 를 '대한민국'으로 설정하여 실행합니다.\n",
        "# chain.invoke({\"country\": \"대한민국\"})\n",
        "print(chain.invoke({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"}))"
      ],
      "metadata": {
        "id": "NqSS-6PhuUin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({\"question\": \"미국에서 피자 주문\"}))"
      ],
      "metadata": {
        "id": "nTiI7hbmvl5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ebook에 있는 다른 예제도 실행해본다."
      ],
      "metadata": {
        "id": "fz-4MOAHv66A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt 를 PromptTemplate 객체로 생성합니다.\n",
        "prompt = PromptTemplate.from_template(\"{topic} 에 대해 쉽게 설명해주세요.\")\n",
        "\n",
        "# input 딕셔너리에 주제를 'ice cream'으로 설정합니다.\n",
        "input = {\"topic\": \"양자역학\"}\n",
        "\n",
        "# prompt 객체의 invoke 메서드를 사용하여 input을 전달하고 대화형 프롬프트 값을 생성합니다.\n",
        "prompt.invoke(input)\n",
        "\n",
        "# prompt 객체와 model 객체를 파이프(|) 연산자로 연결하고 invoke 메서드를 사용하여 input을 전달합니다.\n",
        "# 이를 통해 AI 모델이 생성한 메시지를 반환합니다.\n",
        "(prompt | model).invoke(input)"
      ],
      "metadata": {
        "id": "FuE8r61Hv9uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse_output 메서드를 사용하여 AI 모델이 생성한 메시지 문자열로 출력합니다.\n",
        "display((prompt | model | output_parser).invoke(input))"
      ],
      "metadata": {
        "id": "KBkljGX_wIvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LCEL 인터페이스에 있는 다른 내용은 필요할 때 보면 될 것 같고, 병렬성 예제를 돌려본다."
      ],
      "metadata": {
        "id": "EDuG92JIwqy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "# {country} 의 수도를 물어보는 체인을 생성합니다.\n",
        "chain1 = ChatPromptTemplate.from_template(\"{country} 의 수도는 어디야?\") | model\n",
        "\n",
        "# {country} 의 면적을 물어보는 체인을 생성합니다.\n",
        "chain2 = ChatPromptTemplate.from_template(\"{country} 의 면적은 얼마야?\") | model\n",
        "# 위의 2개 체인을 동시에 생성하는 병렬 실행 체인을 생성합니다.\n",
        "combined = RunnableParallel(capital=chain1, area=chain2)"
      ],
      "metadata": {
        "id": "gmhhArJ6w1No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke(\n",
        "    {\"country\": \"대한민국\"}\n",
        ")"
      ],
      "metadata": {
        "id": "nqfeWAwVxESf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain2.invoke({\"country\": \"미국\"})"
      ],
      "metadata": {
        "id": "urii-7RHxLed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 주어진 'country'에 대해 'combined' 객체의 'invoke' 메서드를 호출합니다.\n",
        "combined.invoke({\"country\": \"대한민국\"})"
      ],
      "metadata": {
        "id": "N5080IAqxQMy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}